evaluating opt-13b.onnx
time: 82.25732970237732
bleu: 0.0
Filename: benchmark.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    83  53925.7 MiB  53925.7 MiB           1   @profile
    84                                         def run_eval(inference_params, dataset, num_samples):
    85  53925.7 MiB      0.0 MiB           1       metrics = {
    86  53925.7 MiB      0.0 MiB           1           "bleu_metric": BLEUScore(),
    87  53925.7 MiB      0.0 MiB           1           "time_metric": 0,
    88                                             }
    89  53925.7 MiB      0.0 MiB           1       np.random.seed(123)
    90  53925.7 MiB      0.0 MiB           2       random_indices = np.random.choice(
    91  53925.7 MiB      0.0 MiB           1           len(dataset), size=num_samples, replace=False)
    92  53927.2 MiB     -0.2 MiB          11       for i in random_indices:
    93  53927.2 MiB     -0.2 MiB          10           prefix, target = get_prefix_target(dataset, i)
    94  53927.2 MiB     -0.2 MiB          10           inference_params["prefix"] = prefix
    95  53927.2 MiB      1.3 MiB          10           update_metrics(inference_params, metrics, target)
    96  53927.2 MiB     -0.0 MiB           1       report_metrics(metrics)


evaluating oopt-13b.onnx
time: 94.27419018745422
bleu: 0.0
Filename: benchmark.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    83  53950.9 MiB  53950.9 MiB           1   @profile
    84                                         def run_eval(inference_params, dataset, num_samples):
    85  53950.9 MiB      0.0 MiB           1       metrics = {
    86  53950.9 MiB      0.0 MiB           1           "bleu_metric": BLEUScore(),
    87  53950.9 MiB      0.0 MiB           1           "time_metric": 0,
    88                                             }
    89  53950.9 MiB      0.0 MiB           1       np.random.seed(123)
    90  53950.9 MiB      0.0 MiB           2       random_indices = np.random.choice(
    91  53950.9 MiB      0.0 MiB           1           len(dataset), size=num_samples, replace=False)
    92  53961.3 MiB      0.0 MiB          11       for i in random_indices:
    93  53961.3 MiB      0.0 MiB          10           prefix, target = get_prefix_target(dataset, i)
    94  53961.3 MiB      0.0 MiB          10           inference_params["prefix"] = prefix
    95  53961.3 MiB     10.4 MiB          10           update_metrics(inference_params, metrics, target)
    96  53961.3 MiB      0.0 MiB           1       report_metrics(metrics)


evaluating foopt-13b.onnx
time: 85.22929644584656
bleu: 0.0
Filename: benchmark.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    83  53946.0 MiB  53946.0 MiB           1   @profile
    84                                         def run_eval(inference_params, dataset, num_samples):
    85  53946.0 MiB      0.0 MiB           1       metrics = {
    86  53946.0 MiB      0.0 MiB           1           "bleu_metric": BLEUScore(),
    87  53946.0 MiB      0.0 MiB           1           "time_metric": 0,
    88                                             }
    89  53946.0 MiB      0.0 MiB           1       np.random.seed(123)
    90  53946.0 MiB      0.0 MiB           2       random_indices = np.random.choice(
    91  53946.0 MiB      0.0 MiB           1           len(dataset), size=num_samples, replace=False)
    92  53949.8 MiB      0.0 MiB          11       for i in random_indices:
    93  53949.8 MiB      0.0 MiB          10           prefix, target = get_prefix_target(dataset, i)
    94  53949.8 MiB      0.0 MiB          10           inference_params["prefix"] = prefix
    95  53949.8 MiB      3.8 MiB          10           update_metrics(inference_params, metrics, target)
    96  53949.8 MiB      0.0 MiB           1       report_metrics(metrics)


evaluating qoopt-13b.onnx
time: 75.48966932296753
bleu: 0.0
Filename: benchmark.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    83  43700.2 MiB  43700.2 MiB           1   @profile
    84                                         def run_eval(inference_params, dataset, num_samples):
    85  43700.2 MiB      0.0 MiB           1       metrics = {
    86  43700.2 MiB      0.0 MiB           1           "bleu_metric": BLEUScore(),
    87  43700.2 MiB      0.0 MiB           1           "time_metric": 0,
    88                                             }
    89  43700.2 MiB      0.0 MiB           1       np.random.seed(123)
    90  43700.2 MiB      0.0 MiB           2       random_indices = np.random.choice(
    91  43700.2 MiB      0.0 MiB           1           len(dataset), size=num_samples, replace=False)
    92  43700.3 MiB      0.0 MiB          11       for i in random_indices:
    93  43700.3 MiB      0.0 MiB          10           prefix, target = get_prefix_target(dataset, i)
    94  43700.3 MiB      0.0 MiB          10           inference_params["prefix"] = prefix
    95  43700.3 MiB      0.1 MiB          10           update_metrics(inference_params, metrics, target)
    96  43700.3 MiB      0.0 MiB           1       report_metrics(metrics)


evaluating baseline
time: 143.8147280216217
bleu: 0.0
Filename: benchmark.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    83  49478.1 MiB  49478.1 MiB           1   @profile
    84                                         def run_eval(inference_params, dataset, num_samples):
    85  49478.1 MiB      0.0 MiB           1       metrics = {
    86  49478.1 MiB      0.0 MiB           1           "bleu_metric": BLEUScore(),
    87  49478.1 MiB      0.0 MiB           1           "time_metric": 0,
    88                                             }
    89  49478.1 MiB      0.0 MiB           1       np.random.seed(123)
    90  49478.1 MiB      0.0 MiB           2       random_indices = np.random.choice(
    91  49478.1 MiB      0.0 MiB           1           len(dataset), size=num_samples, replace=False)
    92  49479.7 MiB      0.0 MiB          11       for i in random_indices:
    93  49479.7 MiB      0.0 MiB          10           prefix, target = get_prefix_target(dataset, i)
    94  49479.7 MiB      0.0 MiB          10           inference_params["prefix"] = prefix
    95  49479.7 MiB      1.5 MiB          10           update_metrics(inference_params, metrics, target)
    96  49479.7 MiB      0.0 MiB           1       report_metrics(metrics)


